{"pageProps":{"post":{"fileName":"2019-06-12-multiple-linear-regression-python.md","fullPath":"/home/runner/work/rusith.me/rusith.me/modules/blog/posts/2019-06-12-multiple-linear-regression-python.md","title":"Multiple Linear Regression With Python","tags":["machineLearning","dataScience","python"],"comments":true,"description":"Regression is a machine learning model which we can use to predict values by using previously observed data. In simple linear regression, we had to use only one independent variable for the prediction. but in the real world often a dependent variable is dependent upon several variables.","dateCreated":"Tue Jun 18 2019 00:00:00 GMT+0000 (Coordinated Universal Time)","dateModified":"Tue Jun 18 2019 00:00:00 GMT+0000 (Coordinated Universal Time)","datePublished":"Tue Jun 18 2019 00:00:00 GMT+0000 (Coordinated Universal Time)","dependencies":"Python","about":"Introduction to Multiple Linear Regression using Python","path":"/blog/multiple-linear-regression-python","oldPath":"/2019/06/12/multiple-linear-regression-python","math":{"mainEq":"$$y =  \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2  + ... \\beta_n x_n$$","beta0":"\\(\\beta_0\\)","beta1n":"\\(\\beta_1 - n\\)"},"dateCreatedFormatted":"Tue, Jun 18, 2019","fullUrl":"https://rusith.me/blog/multiple-linear-regression-python","fileContent":"---\ntitle: Multiple Linear Regression With Python\ntags: machineLearning dataScience python\ncomments: true\ndescription: Regression is a machine learning model which we can use to predict values by using previously observed data. In simple linear regression, we had to use only one independent variable for the prediction. but in the real world often a dependent variable is dependent upon several variables.\ndateCreated: 2019-06-18\ndateModified: 2019-06-18\ndatePublished: 2019-06-18\ndependencies: Python\nabout: Introduction to Multiple Linear Regression using Python\npath: /multiple-linear-regression-python\noldPath: /2019/06/12/multiple-linear-regression-python\nmath:\n  mainEq: '$$y =  \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2  + ... \\beta_n x_n$$'\n  beta0: \\(\\beta_0\\)\n  beta1n: \\(\\beta_1 - n\\)\n---\n\n<div class=\"$$styles.linksBox\">\n  <span class=\"$$styles.alsoRead\">Also read</span>\n  <p><a target=\"_blank\" href=\"$$base_url/2019/05/05/simple-linear-regression-with-r/\">Simple Linear Regression</a></p>\n</div>\n\nRegression is a machine learning model which we can use to predict values by using previously observed data. In simple linear regression, we had to use only one independent variable for the prediction. but in the real world often a dependent variable is dependent upon several variables. so we can't use the simple linear regression model in those cases.\n\nThis is where the multiple linear regression model comes in. it allows you to use multiple independent variables to predict the dependent variable.\nThe formula for the multiple linear regression is given below.\n\n<div style=\"font-size:30px;\">\n$$math-mainEq\n</div>\n\nHere $$math-beta0 is the constant and $$math-beta1n are the coefficients that the model will have to figure out throughout the learning process.\n\nThe data set that we are going to use in this example is a data set which contains the spending and profit data of some companies. we can use multiple linear regression to identify the correlation of the spending to the profit and predict for a new value set. you can download the data set from [here]($$base_url/post-data/2019-06-12-multiple-linear-regression-with-python/startups.csv).\n\nlet's start by importing the required modules\n\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nimport statsmodels.formula.api as s\n```\n\nNow we are ready to read our data set and clean it up to proceed further. and we will also split the data set so we can feed the data to the model right away.\n\n```python\ndataset = pd.read_csv('startups.csv')\n\nX = dataset.iloc[:, :-1].values\ny = dataset.iloc[:, 4].values\n\n# Encoding categories\nct = ColumnTransformer(\n    [('one_hot_encoder', OneHotEncoder(), [3])],\n    remainder='passthrough',\n)\n\nX = ct.fit_transform(X)\n\n# Removing the extra dummy variable\nX = X[:, 1:]\n\n# Splitting the set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n```\n\nOkay. with this done, we have split the data set and can be used to feed to our regression model.\nIn Python for regression, we can use the scikit-learn library, let's do in now.\n\n```python\n# Create the regression model\nregressor = LinearRegression()\n# Fit the model for training data\nregressor.fit(X_train, y_train)\n# Predict for the test data\ny_pred = regressor.predict(X_test)\n```\n\nDone. so now you can compare `y_pred` with `y_test` to see how our regression model has performed.\n\nIn order to use multiple linear regression on a data set. we have to make some assumptions about the data set. these assumptions must be true before applying the regression model on the data set.\n\nFirst one is that the independent variables and the dependent variable must be linearly correlated. meaning the relationship of each independent variable must be a linear one. this can be tested using scatter plots. See the example Linear relationship (Left) and the Non-linear relationship visualized using scatter charts\n\n<img alt=\"Linear data\" src=\"$$base_url/post-data/2019-06-12-multiple-linear-regression-with-python/scatter-linear.png\" style=\"width:250px;float:left; margin-right:20px\" >\n<img alt=\"Non linear data\" src=\"$$base_url/post-data/2019-06-12-multiple-linear-regression-with-python/s-non-linear.png\" style=\"width:250px\">\n\nThe second assumption we have to make is that the residuals of the regression should follow a normal distribution. Residuals are the error terms or the difference between observed and predicted values of the dependent variable. A histogram of the residuals can be used to check that they are normally distributed. Additionally, you can use a P-P Plot to see the normality.\n\nThe third assumption is that homoscedasticity. If you take independent values and residuals, there should be no clear pattern in the distribution.\n\nThe fourth assumption is that the residuals must not be correlated\n\nThe fifth assumption is that the Independent variables should not be correlated too much that those relationships can affect the model.\n","parsedContent":"<div class=\"Post_linksBox__gMwVv\">\n  <span class=\"Post_alsoRead__efgln\">Also read</span>\n  <p><a target=\"_blank\" href=\"https://rusith.me/2019/05/05/simple-linear-regression-with-r/\">Simple Linear Regression</a></p>\n</div>\n<p>Regression is a machine learning model which we can use to predict values by using previously observed data. In simple linear regression, we had to use only one independent variable for the prediction. but in the real world often a dependent variable is dependent upon several variables. so we can't use the simple linear regression model in those cases.</p>\n<p>This is where the multiple linear regression model comes in. it allows you to use multiple independent variables to predict the dependent variable.\nThe formula for the multiple linear regression is given below.</p>\n<div style=\"font-size:30px;\">\n$$y =  \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2  + ... \\beta_n x_n$$\n</div>\n<p>Here \\(\\beta_0\\) is the constant and \\(\\beta_1 - n\\) are the coefficients that the model will have to figure out throughout the learning process.</p>\n<p>The data set that we are going to use in this example is a data set which contains the spending and profit data of some companies. we can use multiple linear regression to identify the correlation of the spending to the profit and predict for a new value set. you can download the data set from <a href=\"https://rusith.me/post-data/2019-06-12-multiple-linear-regression-with-python/startups.csv\">here</a>.</p>\n<p>let's start by importing the required modules</p>\n<pre><code class=\"hljs python language-python\"><span class=\"hljs-keyword\">import</span> numpy <span class=\"hljs-keyword\">as</span> np\n<span class=\"hljs-keyword\">import</span> pandas <span class=\"hljs-keyword\">as</span> pd\n<span class=\"hljs-keyword\">from</span> sklearn.preprocessing <span class=\"hljs-keyword\">import</span> OneHotEncoder\n<span class=\"hljs-keyword\">from</span> sklearn.compose <span class=\"hljs-keyword\">import</span> ColumnTransformer\n<span class=\"hljs-keyword\">from</span> sklearn.model_selection <span class=\"hljs-keyword\">import</span> train_test_split\n<span class=\"hljs-keyword\">from</span> sklearn.linear_model <span class=\"hljs-keyword\">import</span> LinearRegression\n<span class=\"hljs-keyword\">import</span> statsmodels.formula.api <span class=\"hljs-keyword\">as</span> s\n</code></pre>\n<p>Now we are ready to read our data set and clean it up to proceed further. and we will also split the data set so we can feed the data to the model right away.</p>\n<pre><code class=\"hljs python language-python\">dataset = pd.read_csv(<span class=\"hljs-string\">&#x27;startups.csv&#x27;</span>)\n\nX = dataset.iloc[:, :-<span class=\"hljs-number\">1</span>].values\ny = dataset.iloc[:, <span class=\"hljs-number\">4</span>].values\n\n<span class=\"hljs-comment\"># Encoding categories</span>\nct = ColumnTransformer(\n    [(<span class=\"hljs-string\">&#x27;one_hot_encoder&#x27;</span>, OneHotEncoder(), [<span class=\"hljs-number\">3</span>])],\n    remainder=<span class=\"hljs-string\">&#x27;passthrough&#x27;</span>,\n)\n\nX = ct.fit_transform(X)\n\n<span class=\"hljs-comment\"># Removing the extra dummy variable</span>\nX = X[:, <span class=\"hljs-number\">1</span>:]\n\n<span class=\"hljs-comment\"># Splitting the set</span>\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = <span class=\"hljs-number\">0.2</span>)\n</code></pre>\n<p>Okay. with this done, we have split the data set and can be used to feed to our regression model.\nIn Python for regression, we can use the scikit-learn library, let's do in now.</p>\n<pre><code class=\"hljs python language-python\"><span class=\"hljs-comment\"># Create the regression model</span>\nregressor = LinearRegression()\n<span class=\"hljs-comment\"># Fit the model for training data</span>\nregressor.fit(X_train, y_train)\n<span class=\"hljs-comment\"># Predict for the test data</span>\ny_pred = regressor.predict(X_test)\n</code></pre>\n<p>Done. so now you can compare <code>y_pred</code> with <code>y_test</code> to see how our regression model has performed.</p>\n<p>In order to use multiple linear regression on a data set. we have to make some assumptions about the data set. these assumptions must be true before applying the regression model on the data set.</p>\n<p>First one is that the independent variables and the dependent variable must be linearly correlated. meaning the relationship of each independent variable must be a linear one. this can be tested using scatter plots. See the example Linear relationship (Left) and the Non-linear relationship visualized using scatter charts</p>\n<p><img alt=\"Linear data\" src=\"https://rusith.me/post-data/2019-06-12-multiple-linear-regression-with-python/scatter-linear.png\" style=\"width:250px;float:left; margin-right:20px\" >\n<img alt=\"Non linear data\" src=\"https://rusith.me/post-data/2019-06-12-multiple-linear-regression-with-python/s-non-linear.png\" style=\"width:250px\"></p>\n<p>The second assumption we have to make is that the residuals of the regression should follow a normal distribution. Residuals are the error terms or the difference between observed and predicted values of the dependent variable. A histogram of the residuals can be used to check that they are normally distributed. Additionally, you can use a P-P Plot to see the normality.</p>\n<p>The third assumption is that homoscedasticity. If you take independent values and residuals, there should be no clear pattern in the distribution.</p>\n<p>The fourth assumption is that the residuals must not be correlated</p>\n<p>The fifth assumption is that the Independent variables should not be correlated too much that those relationships can affect the model.</p>"},"page":"Post","topTags":["programming","aws","web","machineLearning","react","javascript","dataScience","typescript","r","csharp"],"relatedPosts":[{"title":"Simple Linear Regression With R","tags":["machineLearning","dataScience","r"],"fullUrl":"https://rusith.me/blog/simple-linear-regression-with-r","date":"Tue, May 7, 2019","description":"Regression models are used to predict real values such as salary, spending, income. Simple linear regression is a model of regression which is used to identify the correlation between two variables and possibly predict the dependent variable by using the independent variable.This will enable us to establish a relationship between two attributes such as Income and Spending and we can use what we know about the relationship to forecast unobserved values.","banner":null},{"title":"Data Pre-Processing With R","tags":["machineLearning","dataScience","r"],"fullUrl":"https://rusith.me/blog/data-preprocessing-with-r","date":"Sat, May 4, 2019","description":"Before feeding our dataSet into a machine learning algorithm it's absolutely necessary to pre-process the data where we should clean and re-shape our data to get the maximum performance from our machine learning models. In this post, I will go through a set of procedures which you can use to pre-process a data set.","banner":null},{"title":"The Essence of Machine Learning","tags":["machineLearning"],"fullUrl":"https://rusith.me/blog/the-essence-of-machine-learning","date":"Tue, May 7, 2019","description":"Machine learning can significantly improve the performance of a system. This could be very beneficial for businesses or any other application. So if someone comes to you and suggest you a business idea that involves machine learning, how do you decide that a machine learning implementation is feasible with the particular idea?","banner":null}]},"__N_SSG":true}